{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bite249004696c34343bfa65420b07328ed",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting flask==1.1.2\n",
      "  Using cached Flask-1.1.2-py2.py3-none-any.whl (94 kB)\n",
      "Collecting flask_restful\n",
      "  Using cached Flask_RESTful-0.3.8-py2.py3-none-any.whl (25 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-1.1.4-cp38-cp38-manylinux1_x86_64.whl (9.3 MB)\n",
      "Collecting unidecode\n",
      "  Using cached Unidecode-1.1.1-py2.py3-none-any.whl (238 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "Collecting language-check\n",
      "  Using cached language-check-1.1.tar.gz (33 kB)\n",
      "Collecting pycontractions\n",
      "  Using cached pycontractions-2.0.1-py3-none-any.whl (9.6 kB)\n",
      "Collecting pyspellchecker\n",
      "  Using cached pyspellchecker-0.5.5-py2.py3-none-any.whl (1.9 MB)\n",
      "Collecting gensim\n",
      "  Using cached gensim-3.8.3-cp38-cp38-manylinux1_x86_64.whl (24.2 MB)\n",
      "Collecting contractions\n",
      "  Using cached contractions-0.0.25-py2.py3-none-any.whl (3.2 kB)\n",
      "Collecting lxml\n",
      "  Using cached lxml-4.6.1-cp38-cp38-manylinux1_x86_64.whl (5.4 MB)\n",
      "Collecting spacy\n",
      "  Using cached spacy-2.3.2-cp38-cp38-manylinux1_x86_64.whl (9.8 MB)\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Collecting en_core_web_lg\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.3.1/en_core_web_lg-2.3.1.tar.gz (782.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 782.7 MB 10 kB/s \n",
      "\u001b[?25hCollecting en_core_web_sm\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 12.0 MB/s \n",
      "\u001b[?25hCollecting spark-nlp\n",
      "  Using cached spark_nlp-2.6.3-py2.py3-none-any.whl (129 kB)\n",
      "Requirement already satisfied: click>=5.1 in /usr/lib/python3/dist-packages (from flask==1.1.2->-r /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/requirements.txt (line 1)) (7.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /home/sandipan/.local/lib/python3.8/site-packages (from flask==1.1.2->-r /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/requirements.txt (line 1)) (2.11.2)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Using cached itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting Werkzeug>=0.15\n",
      "  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n",
      "Collecting pytz\n",
      "  Using cached pytz-2020.4-py2.py3-none-any.whl (509 kB)\n",
      "Requirement already satisfied: six>=1.3.0 in /usr/lib/python3/dist-packages (from flask_restful->-r /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/requirements.txt (line 2)) (1.14.0)\n",
      "Collecting aniso8601>=0.82\n",
      "  Using cached aniso8601-8.0.0-py2.py3-none-any.whl (43 kB)\n",
      "Collecting numpy>=1.15.4\n",
      "  Using cached numpy-1.19.4-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/sandipan/.local/lib/python3.8/site-packages (from pandas->-r /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/requirements.txt (line 3)) (2.8.1)\n",
      "Collecting soupsieve>1.2; python_version >= \"3.0\"\n",
      "  Using cached soupsieve-2.0.1-py3-none-any.whl (32 kB)\n",
      "Collecting pyemd>=0.4.4\n",
      "  Using cached pyemd-0.5.1.tar.gz (91 kB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-3.0.0.tar.gz (113 kB)\n",
      "Collecting scipy>=0.18.1\n",
      "  Using cached scipy-1.5.3-cp38-cp38-manylinux1_x86_64.whl (25.8 MB)\n",
      "Collecting textsearch\n",
      "  Using cached textsearch-0.0.17-py2.py3-none-any.whl (7.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.3-cp38-cp38-manylinux2014_x86_64.whl (130 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Using cached tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Using cached srsly-1.0.2-cp38-cp38-manylinux1_x86_64.whl (185 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy->-r /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/requirements.txt (line 12)) (45.2.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.4-cp38-cp38-manylinux2014_x86_64.whl (20 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy->-r /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/requirements.txt (line 12)) (2.22.0)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.4-cp38-cp38-manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Using cached blis-0.4.1-cp38-cp38-manylinux1_x86_64.whl (3.7 MB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Using cached wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Collecting thinc==7.4.1\n",
      "  Using cached thinc-7.4.1-cp38-cp38-manylinux1_x86_64.whl (2.1 MB)\n",
      "Collecting nltk>=3.1\n",
      "  Using cached nltk-3.5.zip (1.4 MB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/sandipan/.local/lib/python3.8/site-packages (from Jinja2>=2.10.1->flask==1.1.2->-r /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/requirements.txt (line 1)) (1.1.1)\n",
      "Collecting pyahocorasick\n",
      "  Using cached pyahocorasick-1.4.0.tar.gz (312 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-0.17.0-py3-none-any.whl (301 kB)\n",
      "Collecting regex\n",
      "  Using cached regex-2020.10.28-cp38-cp38-manylinux2014_x86_64.whl (736 kB)\n",
      "Building wheels for collected packages: language-check, en-core-web-lg, en-core-web-sm, pyemd, smart-open, nltk, pyahocorasick\n",
      "  Building wheel for language-check (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for language-check: filename=language_check-1.1-py3-none-any.whl size=56968213 sha256=7a653430dc5ac86d26556d02bf92d694d58488c9fa42b5b67a9624c087b54803\n",
      "  Stored in directory: /home/sandipan/.cache/pip/wheels/da/c5/78/e19f0e3afc153a6fffa88abf7d12321a8f8b43e1faa3fc590b\n",
      "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.3.1-py3-none-any.whl size=782936123 sha256=07d46861d15eeeee5104162bc58cd029ad5052587baec5fc66b3cca04c1f8323\n",
      "  Stored in directory: /home/sandipan/.cache/pip/wheels/8b/bb/bb/bdc918f4b37d930a1be9ed876e7b2c2ee518a34803d78a248e\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=b1bc9ee924afeb985c4db0e5085889a7115a9cb3c35ee7a7617717f0357500b8\n",
      "  Stored in directory: /home/sandipan/.cache/pip/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
      "  Building wheel for pyemd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyemd: filename=pyemd-0.5.1-cp38-cp38-linux_x86_64.whl size=572478 sha256=ec31e92270df84e49299f18b288e012707e7a8f8861219878f4754b74c175790\n",
      "  Stored in directory: /home/sandipan/.cache/pip/wheels/a2/a5/34/f960a47ca5c06b0e91b6f48117a79a66f53a879f8fac9529bf\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107092 sha256=9d32bd91f17f96a7222b8e476ed9d99e6c8952579ce4fbda00e397570f7aac2b\n",
      "  Stored in directory: /home/sandipan/.cache/pip/wheels/11/73/9a/f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=04d5b3f2e0ecf0813b401ab73b2d517683e0386bbb09827e4ccc97e30084e659\n",
      "  Stored in directory: /home/sandipan/.cache/pip/wheels/ff/d5/7b/f1fb4e1e1603b2f01c2424dd60fbcc50c12ef918bafc44b155\n",
      "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp38-cp38-linux_x86_64.whl size=94509 sha256=b1a46e1ae2b74b364684f2e536a5dfb1722c34234a9821d4d253aae0a00c0dc2\n",
      "  Stored in directory: /home/sandipan/.cache/pip/wheels/d0/72/5e/5af4d48e71e0b4dc197445664eb1719fe9c9b6fcead4930595\n",
      "Successfully built language-check en-core-web-lg en-core-web-sm pyemd smart-open nltk pyahocorasick\n",
      "Installing collected packages: itsdangerous, Werkzeug, flask, pytz, aniso8601, flask-restful, numpy, pandas, unidecode, soupsieve, beautifulsoup4, language-check, pyemd, smart-open, scipy, gensim, pycontractions, pyspellchecker, pyahocorasick, textsearch, contractions, lxml, cymem, murmurhash, preshed, plac, tqdm, srsly, catalogue, blis, wasabi, thinc, spacy, joblib, regex, nltk, textblob, en-core-web-lg, en-core-web-sm, spark-nlp\n",
      "Successfully installed Werkzeug-1.0.1 aniso8601-8.0.0 beautifulsoup4-4.9.3 blis-0.4.1 catalogue-1.0.0 contractions-0.0.25 cymem-2.0.4 en-core-web-lg-2.3.1 en-core-web-sm-2.3.1 flask-1.1.2 flask-restful-0.3.8 gensim-3.8.3 itsdangerous-1.1.0 joblib-0.17.0 language-check-1.1 lxml-4.6.1 murmurhash-1.0.4 nltk-3.5 numpy-1.19.4 pandas-1.1.4 plac-1.1.3 preshed-3.0.3 pyahocorasick-1.4.0 pycontractions-2.0.1 pyemd-0.5.1 pyspellchecker-0.5.5 pytz-2020.4 regex-2020.10.28 scipy-1.5.3 smart-open-3.0.0 soupsieve-2.0.1 spacy-2.3.2 spark-nlp-2.6.3 srsly-1.0.2 textblob-0.15.3 textsearch-0.0.17 thinc-7.4.1 tqdm-4.51.0 unidecode-1.1.1 wasabi-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[nltk_data] Downloading package brown to /home/sandipan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/sandipan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sandipan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sandipan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /home/sandipan/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/sandipan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting textacy\n",
      "  Downloading textacy-0.10.1-py3-none-any.whl (183 kB)\n",
      "\u001b[K     |████████████████████████████████| 183 kB 3.1 MB/s \n",
      "\u001b[?25hCollecting pyphen>=0.9.4\n",
      "  Downloading Pyphen-0.10.0-py3-none-any.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 8.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.0.0,>=2.2.0 in /home/sandipan/.local/lib/python3.8/site-packages (from textacy) (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/sandipan/.local/lib/python3.8/site-packages (from textacy) (1.19.4)\n",
      "Collecting cytoolz>=0.8.0\n",
      "  Downloading cytoolz-0.11.0.tar.gz (477 kB)\n",
      "\u001b[K     |████████████████████████████████| 477 kB 12.0 MB/s \n",
      "\u001b[?25hCollecting cachetools>=2.0.1\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting jellyfish>=0.7.0\n",
      "  Downloading jellyfish-0.8.2-cp38-cp38-manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 3.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /home/sandipan/.local/lib/python3.8/site-packages (from textacy) (1.5.3)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /home/sandipan/.local/lib/python3.8/site-packages (from textacy) (0.17.0)\n",
      "Requirement already satisfied: srsly>=0.0.5 in /home/sandipan/.local/lib/python3.8/site-packages (from textacy) (1.0.2)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /home/sandipan/.local/lib/python3.8/site-packages (from textacy) (4.51.0)\n",
      "Requirement already satisfied: pyemd>=0.5.0 in /home/sandipan/.local/lib/python3.8/site-packages (from textacy) (0.5.1)\n",
      "Collecting networkx>=2.0\n",
      "  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 12.0 MB/s \n",
      "\u001b[?25hCollecting scikit-learn<0.24.0,>=0.19.0\n",
      "  Downloading scikit_learn-0.23.2-cp38-cp38-manylinux1_x86_64.whl (6.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.8 MB 11.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/lib/python3/dist-packages (from textacy) (2.22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sandipan/.local/lib/python3.8/site-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sandipan/.local/lib/python3.8/site-packages (from spacy<3.0.0,>=2.2.0->textacy) (2.0.4)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/sandipan/.local/lib/python3.8/site-packages (from spacy<3.0.0,>=2.2.0->textacy) (7.4.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.0.0,>=2.2.0->textacy) (45.2.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/sandipan/.local/lib/python3.8/site-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/sandipan/.local/lib/python3.8/site-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.8.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/sandipan/.local/lib/python3.8/site-packages (from spacy<3.0.0,>=2.2.0->textacy) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/sandipan/.local/lib/python3.8/site-packages (from spacy<3.0.0,>=2.2.0->textacy) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sandipan/.local/lib/python3.8/site-packages (from spacy<3.0.0,>=2.2.0->textacy) (3.0.3)\n",
      "Collecting toolz>=0.8.0\n",
      "  Downloading toolz-0.11.1-py3-none-any.whl (55 kB)\n",
      "\u001b[K     |████████████████████████████████| 55 kB 5.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /home/sandipan/.local/lib/python3.8/site-packages (from networkx>=2.0->textacy) (4.4.2)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
      "Building wheels for collected packages: cytoolz\n",
      "  Building wheel for cytoolz (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cytoolz: filename=cytoolz-0.11.0-cp38-cp38-linux_x86_64.whl size=1912181 sha256=cff2f0cbaac6320227c4450911448c1e01c5588c4184e2ad35e0072ee3680622\n",
      "  Stored in directory: /home/sandipan/.cache/pip/wheels/12/80/98/b62b7fba2c0b84c34d515324548a3205c078ae0109293fb617\n",
      "Successfully built cytoolz\n",
      "Installing collected packages: pyphen, toolz, cytoolz, cachetools, jellyfish, networkx, threadpoolctl, scikit-learn, textacy\n",
      "Successfully installed cachetools-4.1.1 cytoolz-0.11.0 jellyfish-0.8.2 networkx-2.5 pyphen-0.10.0 scikit-learn-0.23.2 textacy-0.10.1 threadpoolctl-2.1.0 toolz-0.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy.ke\n",
    "from textacy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a spacy model, which will be used for all further processing.\n",
    "en = textacy.load_spacy_lang(\"en_core_web_sm\", disable=(\"parser\",))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/test.text\"\n",
    "mytext = open(file_path).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to spacy doc\n",
    "doc = textacy.make_spacy_doc(mytext, lang=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Doc(91 tokens: \"Since the so-called \\\\\"statistical revolution\\\\\" ...\")'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# preview the data\n",
    "doc._.preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Natural Language Processing research', 0.057445694944219174),\n",
       " ('natural language variation', 0.0429747524435442),\n",
       " ('direct hand coding', 0.03577846846720794),\n",
       " ('machine learning', 0.032126592569189094),\n",
       " ('late 1980', 0.025545484903939483)]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# get the keyword extract\n",
    "textacy.ke.textrank(doc, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('Natural Language Processing research', 0.057445694944219174),\n",
       " ('natural language variation', 0.0429747524435442),\n",
       " ('direct hand coding', 0.03577846846720794),\n",
       " ('machine learning', 0.032126592569189094),\n",
       " ('late 1980', 0.025545484903939483),\n",
       " ('mid 1990', 0.025541248816383998),\n",
       " ('\\\\\"statistical revolution\\\\', 0.025511744654411136),\n",
       " ('statistical inference', 0.02535959044963539),\n",
       " ('general robust', 0.02407624703107341),\n",
       " ('world example', 0.02307085146131378)]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "textacy.ke.textrank(doc, normalize=\"lemma\",topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Textrank output:  ['\\\\\"statistical revolution\\\\', 'Natural Language Processing research', 'natural language variation', 'late 1980', 'direct hand coding']\nTextrank output:  ['Natural Language Processing research', 'natural language variation', 'direct hand coding', 'machine learning', 'late 1980']\nSGRank output:  ['Natural Language Processing research', 'direct hand coding', 'natural language variation', 'mid 1990', 'machine learning']\nSGRank output:  ['Natural Language Processing research', 'direct hand coding', 'natural language variation', 'mid 1990', 'machine learning']\nSGRank output:  ['machine learning']\nSGRank output:  ['Processing research', 'language variation', 'hand coding', 'Language Processing', 'natural language']\n"
     ]
    }
   ],
   "source": [
    "#Print the keywords using TextRank algorithm, as implemented in Textacy.\n",
    "print(\"Textrank output: \", [kps for kps, weights in textacy.ke.textrank(doc, normalize=\"lemma\", window_size = 5, position_bias = True, topn=5)])\n",
    "print(\"Textrank output: \", [kps for kps, weights in textacy.ke.textrank(doc, normalize=\"lemma\", topn=5)]) #use this\n",
    "#Print the key words and phrases, using SGRank algorithm, as implemented in Textacy\n",
    "print(\"SGRank output: \", [kps for kps, weights in textacy.ke.sgrank(doc, topn=5)])\n",
    "print(\"SGRank output: \", [kps for kps, weights in textacy.ke.sgrank(doc, topn=5, normalize=\"lemma\")]) #use this\n",
    "print(\"SGRank output: \", [kps for kps, weights in textacy.ke.sgrank(doc, normalize=\"lemma\", window_size= 2, topn=5)])\n",
    "print(\"SGRank output: \", [kps for kps, weights in textacy.ke.sgrank(doc, normalize=\"lemma\", ngrams = 2, window_size= 3, topn=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'Natural Language Processing research'}, {'natural language variation'}, {'\\\\\"statistical revolution\\\\'}, {'statistical inference'}, {'direct hand coding'}, {'machine learning'}, {'general robust'}, {'late 1980'}, {'mid 1990'}, {'rule'}]\n"
     ]
    }
   ],
   "source": [
    "#To address the issue of overlapping key phrases, textacy has a function: aggregage_term_variants.\n",
    "#Choosing one of the grouped terms per item will give us a list of non-overlapping key phrases!\n",
    "terms = set([term for term,weight in textacy.ke.sgrank(doc)])\n",
    "print(textacy.ke.utils.aggregate_term_variants(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'Natural Language Processing research'}, {'natural language variation'}, {'direct hand coding'}, {'machine learning'}, {'late 1980'}]\n"
     ]
    }
   ],
   "source": [
    "#To address the issue of overlapping key phrases, textacy has a function: aggregage_term_variants.\n",
    "#Choosing one of the grouped terms per item will give us a list of non-overlapping key phrases!\n",
    "terms = set([term for term,weight in textacy.ke.textrank(doc, normalize=\"lemma\", topn=5)])\n",
    "print(textacy.ke.utils.aggregate_term_variants(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"gopleader trumpliesamericansdie at least   ppl have fly to the us from china since dec   when trump impose travel restriction thousand fly directly from wuhan the original epicenter of the covid outbreak votetrumpout voteoutgop httpstcohqbrczbryw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = textacy.make_spacy_doc(data, lang=en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Textrank output:  ['covid outbreak votetrumpout voteoutgop httpstcohqbrczbryw', 'trump impose travel restriction', 'gopleader trumpliesamericansdie', 'original epicenter', 'fly']\nSGRank output:  ['covid outbreak votetrumpout voteoutgop httpstcohqbrczbryw', 'trump impose travel restriction', 'gopleader trumpliesamericansdie', 'original epicenter', 'dec']\n"
     ]
    }
   ],
   "source": [
    "#Print the keywords using TextRank algorithm, as implemented in Textacy.\n",
    "print(\"Textrank output: \", [kps for kps, weights in textacy.ke.textrank(doc, normalize=\"lemma\", topn=5)]) #use this\n",
    "#Print the key words and phrases, using SGRank algorithm, as implemented in Textacy\n",
    "print(\"SGRank output: \", [kps for kps, weights in textacy.ke.sgrank(doc, topn=5, normalize=\"lemma\")]) #use this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Textrank output:  ['covid outbreak votetrumpout voteoutgop httpstcohqbrczbryw', 'travel restriction thousand', 'gopleader trumpliesamericansdie', 'original epicenter', 'dec']\nSGRank output:  ['covid outbreak votetrumpout voteoutgop httpstcohqbrczbryw', 'travel restriction thousand', 'gopleader trumpliesamericansdie', 'original epicenter', 'dec']\n"
     ]
    }
   ],
   "source": [
    "data = \"gopleader trumpliesamericansdie at least  ppl have flown to the us from china since dec  when trump imposed travel restrictions thousands flew directly from wuhan the original epicenter of the covid outbreak votetrumpout voteoutgop httpstcohqbrczbryw\"\n",
    "doc = textacy.make_spacy_doc(data, lang=en)\n",
    "#Print the keywords using TextRank algorithm, as implemented in Textacy.\n",
    "print(\"Textrank output: \", [kps for kps, weights in textacy.ke.textrank(doc, normalize=\"lemma\", window_size = 2, topn=5)]) #use this\n",
    "#Print the key words and phrases, using SGRank algorithm, as implemented in Textacy\n",
    "print(\"SGRank output: \", [kps for kps, weights in textacy.ke.sgrank(doc, topn=5, normalize=\"lemma\")]) #use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Textrank output:  ['votetrumpout2020 🤜 🏽#VoteOutGOP', 'travel restriction', 'COVID-19 outbreak', 'original epicenter', '@GOPLeader 👟']\nSGRank output:  ['votetrumpout2020 🤜 🏽#VoteOutGOP', 'COVID-19 outbreak', 'travel restriction', '@GOPLeader 👟', 'original epicenter']\n"
     ]
    }
   ],
   "source": [
    "data = \"@GOPLeader 👟#TrumpLiesAmericansDie\\n\\nAt least 430,000 ppl have flown to the US from China since Dec 31 when tRump imposed travel restrictions. \\n\\\"Thousands flew directly from Wuhan\\\" the original epicenter of the COVID-19 outbreak.\\n\\n🗳#VoteTrumpOut2020 🤜🏽#VoteOutGOP\\n\\nhttps://t.co/HqbrczbRyw\"\n",
    "doc = textacy.make_spacy_doc(data, lang=en)\n",
    "#Print the keywords using TextRank algorithm, as implemented in Textacy.\n",
    "print(\"Textrank output: \", [kps for kps, weights in textacy.ke.textrank(doc, normalize=\"lemma\", topn=5)]) #use this\n",
    "#Print the key words and phrases, using SGRank algorithm, as implemented in Textacy\n",
    "print(\"SGRank output: \", [kps for kps, weights in textacy.ke.sgrank(doc, topn=5, normalize=\"lemma\")]) #use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Textrank output:  ['VoteTrumpOut2020 VoteOutGOP https://t.co/HqbrczbRyw', 'travel restriction', 'COVID-19 outbreak', 'original epicenter', 'tRump']\nSGRank output:  ['VoteTrumpOut2020 VoteOutGOP https://t.co/HqbrczbRyw', 'COVID-19 outbreak', 'original epicenter', 'travel restriction', 'tRump']\n"
     ]
    }
   ],
   "source": [
    "data = \"GOPLeader TrumpLiesAmericansDie At least 430,000 ppl have flown to the US from China since Dec 31 when tRump imposed travel restrictions. Thousands flew directly from Wuhan the original epicenter of the COVID-19 outbreak. VoteTrumpOut2020 VoteOutGOP https://t.co/HqbrczbRyw\"\n",
    "doc = textacy.make_spacy_doc(data, lang=en)\n",
    "#Print the keywords using TextRank algorithm, as implemented in Textacy.\n",
    "print(\"Textrank output: \", [kps for kps, weights in textacy.ke.textrank(doc, normalize=\"lemma\", topn=5)]) #use this\n",
    "#Print the key words and phrases, using SGRank algorithm, as implemented in Textacy\n",
    "print(\"SGRank output: \", [kps for kps, weights in textacy.ke.sgrank(doc, topn=5, normalize=\"lemma\")]) #use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gopleader trumpliesamericansdie at least 430,000 ppl have flown to the us from china since dec 31 when trump imposed travel restrictions. thousands flew directly from wuhan the original epicenter of the covid-19 outbreak. votetrumpout2020 voteoutgop\nTextrank output:  ['travel restriction', 'gopleader trumpliesamericansdie', 'covid-19 outbreak', 'original epicenter', 'votetrumpout2020 voteoutgop']\nTextrank output:  ['travel restriction', 'gopleader trumpliesamericansdie', 'covid-19 outbreak', 'original epicenter', 'votetrumpout2020 voteoutgop']\nSGRank output:  ['votetrumpout2020 voteoutgop', 'covid-19 outbreak', 'original epicenter', 'travel restriction', 'gopleader trumpliesamericansdie']\n"
     ]
    }
   ],
   "source": [
    "data = \"GOPLeader TrumpLiesAmericansDie At least 430,000 ppl have flown to the US from China since Dec 31 when tRump imposed travel restrictions. Thousands flew directly from Wuhan the original epicenter of the COVID-19 outbreak. VoteTrumpOut2020 VoteOutGOP\"\n",
    "data = data.lower()\n",
    "print(data)\n",
    "doc = textacy.make_spacy_doc(data, lang=en)\n",
    "#Print the keywords using TextRank algorithm, as implemented in Textacy.\n",
    "print(\"Textrank output: \", [kps for kps, weights in textacy.ke.textrank(doc, normalize=\"lemma\", topn=5)]) #use this\n",
    "print(\"Textrank output: \", [kps for kps, weights in textacy.ke.textrank(doc, topn=5)]) #use this\n",
    "#Print the key words and phrases, using SGRank algorithm, as implemented in Textacy\n",
    "print(\"SGRank output: \", [kps for kps, weights in textacy.ke.sgrank(doc, topn=5, normalize=\"lemma\")]) #use this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from json\n",
    "import pandas as pd\n",
    "import os, json\n",
    "import numpy as np\n",
    "import glob\n",
    "pd.set_option('display.max_columns', None)\n",
    "import string\n",
    "import unidecode\n",
    "import re\n",
    "import time\n",
    "data_file = \"/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/*json\"\n",
    "file_list = glob.glob(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00010-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00007-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00009-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00008-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00012-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00000-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00006-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00002-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00011-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00004-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00001-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00005-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json',\n",
       " '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00003-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00010-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00007-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00009-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00008-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00012-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00000-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00006-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00002-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00011-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00004-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00001-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00005-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n",
      "processing /home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/NLP/data/part-00003-aefb6470-9187-440c-99b3-37c86f3dde14-c000.json\n"
     ]
    }
   ],
   "source": [
    "dfs = [] # an empty list to store the data frames\n",
    "for file in file_list:\n",
    "    print(\"processing %s\" %file)\n",
    "    data = pd.read_json(file, lines=True) # read data frame from json file\n",
    "    dfs.append(data) # append the data frame to the list\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp = pd.concat(dfs, ignore_index=True) # concatenate all the data frames in the list.\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "id                        2164811\n",
       "created_at                2164811\n",
       "emails                    2164811\n",
       "hashtag                   2164811\n",
       "lang                      2164811\n",
       "mention                   2164811\n",
       "source                    2164811\n",
       "user_description          1772989\n",
       "user_id_str               2164811\n",
       "user_location             1439574\n",
       "user_name                 2164811\n",
       "tweet_text                2164277\n",
       "clean_tweet_text          2164811\n",
       "clean_tweet_text_lamma    2164811\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "temp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                         id                created_at emails hashtag lang  \\\n",
       "832668  1247443412988473346 2020-04-07 08:37:40+00:00     []      []   en   \n",
       "\n",
       "             mention                                             source  \\\n",
       "832668  [@GOPLeader]  <a href=\"https://mobile.twitter.com\" rel=\"nofo...   \n",
       "\n",
       "                                         user_description  user_id_str  \\\n",
       "832668  Content Contributor @animaeme striving to defe...    520148825   \n",
       "\n",
       "          user_location user_name  \\\n",
       "832668  California, USA  Animaeme   \n",
       "\n",
       "                                               tweet_text  \\\n",
       "832668  @GOPLeader 👟#TrumpLiesAmericansDie\\n\\nAt least...   \n",
       "\n",
       "                                         clean_tweet_text  \\\n",
       "832668  gopleader trumpliesamericansdie at least  ppl ...   \n",
       "\n",
       "                                   clean_tweet_text_lamma  \n",
       "832668  gopleader trumpliesamericansdie at least   ppl...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>created_at</th>\n      <th>emails</th>\n      <th>hashtag</th>\n      <th>lang</th>\n      <th>mention</th>\n      <th>source</th>\n      <th>user_description</th>\n      <th>user_id_str</th>\n      <th>user_location</th>\n      <th>user_name</th>\n      <th>tweet_text</th>\n      <th>clean_tweet_text</th>\n      <th>clean_tweet_text_lamma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>832668</th>\n      <td>1247443412988473346</td>\n      <td>2020-04-07 08:37:40+00:00</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>en</td>\n      <td>[@GOPLeader]</td>\n      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n      <td>Content Contributor @animaeme striving to defe...</td>\n      <td>520148825</td>\n      <td>California, USA</td>\n      <td>Animaeme</td>\n      <td>@GOPLeader 👟#TrumpLiesAmericansDie\\n\\nAt least...</td>\n      <td>gopleader trumpliesamericansdie at least  ppl ...</td>\n      <td>gopleader trumpliesamericansdie at least   ppl...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "temp[temp['id'] == 1247443412988473346]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    # Remove any web url starting with http or www\n",
    "    text = 'NULL' if text is None else re.sub(r'(http|https|ftp|ssh|sftp|www)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , str(text))\n",
    "    return text\n",
    "\n",
    "def remove_all_punctuation(text):\n",
    "  '''Remove other punctuation, adding fe more\n",
    "  string.punctuation = !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n",
    "  '''\n",
    "  PUNCT_TO_REMOVE = string.punctuation\n",
    "  text = 'NULL' if text is None else str(text).translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "  return text\n",
    "\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    \"\"\"remove accented characters from text, e.g. café\"\"\"\n",
    "    text = 'NULL' if text is None else unidecode.unidecode(str(text))\n",
    "    return text\n",
    "\n",
    "def to_lower(text):\n",
    "  '''conver all to lower'''\n",
    "  text = 'NULL' if text is None else str(text).lower()\n",
    "  return text\n",
    "\n",
    "def remove_tabs(input):\n",
    "    text = 'NULL' if input is None else str(input)\n",
    "    '''remove all the tab, new line char'''\n",
    "    text = text.replace('\\t', ' ')\n",
    "    text = text.replace('\\r', ' ')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "def get_ie_keywords(text):\n",
    "    text = str(text)\n",
    "    doc = textacy.make_spacy_doc(text, lang=en)\n",
    "    Textrank = [kps for kps, weights in textacy.ke.textrank(doc, normalize=\"lemma\", topn=5)] #use this\n",
    "    SGRank = [kps for kps, weights in textacy.ke.sgrank(doc, topn=5, normalize=\"lemma\")] #use this\n",
    "    return Textrank, SGRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- 54.18207812309265 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# clean_data = data['tweet_text'].apply(remove_all_punctuation)\n",
    "start_time = time.time()\n",
    "temp['clean_tweet_ie'] = temp['tweet_text'].apply(remove_url).apply(remove_all_punctuation).apply(remove_accented_chars).apply(to_lower).apply(remove_tabs)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- 22937.265254735947 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "temp['ie_keywords'] = temp['clean_tweet_ie'].apply(get_ie_keywords)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp[temp['id'] == 1247443412988473346]['clean_tweet_ie'].to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}