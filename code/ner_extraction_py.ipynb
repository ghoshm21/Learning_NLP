{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnlpd8d34a4971064c91a824973644f122aa",
   "display_name": "Python 3.8.5 64-bit ('nlp')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020-11-21 20:27:47,031 loading file /home/sandipan/.flair/models/en-ner-ontonotes-fast-v0.4.pt\n",
      "[nltk_data] Downloading package punkt to /home/sandipan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/sandipan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/sandipan/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/sandipan/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "2020-11-21 20:27:51,471 loading file /home/sandipan/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "# import everything\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "import requests\n",
    "from flair.models import SequenceTagger\n",
    "flair_12class = SequenceTagger.load('ner-ontonotes-fast')\n",
    "flair_4class = SequenceTagger.load('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "document= \"so now the fake news nytimes is trace the coronavirus origin back to europe not china this is a first i wonder what the fail new york times get for this one are there any name source they were recently throw out of china like dog and obviously want back in sad\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy models\n",
    "en = None\n",
    "def get_spacy_model():\n",
    "    global en\n",
    "    global _model\n",
    "    if not en:\n",
    "       _model = spacy.load('en_core_web_lg')\n",
    "    en = _model\n",
    "    return en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all NER using SPACY\n",
    "def get_ner_spacy(text):\n",
    "    text = 'NULL' if text is None else str(text)\n",
    "    nlp = get_spacy_model()\n",
    "    ner_tags = {}\n",
    "    if text:        \n",
    "        doc = nlp(text)\n",
    "        for ee in doc.ents:\n",
    "            ner_tags[ee.text] = ee.label_\n",
    "    return ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'nytimes': 'ORG', 'europe': 'LOC', 'china': 'GPE', 'first': 'ORDINAL', 'new york times': 'ORG'}\n"
     ]
    }
   ],
   "source": [
    "# run the spacy code\n",
    "a = get_ner_spacy(document)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all NOUNs and proper nouns POS tag using SPACY\n",
    "def get_noun_spacy(text):\n",
    "    text = 'NULL' if text is None else str(text)\n",
    "    nlp = get_spacy_model()\n",
    "    noun_tags = {}\n",
    "    if text:        \n",
    "        doc = nlp(text)\n",
    "        for token in doc:\n",
    "            if(token.pos_ == 'NOUN' or token.pos_ == 'PROPN'):\n",
    "                noun_tags[token.text] = token.pos_\n",
    "    return noun_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'news': 'NOUN', 'nytimes': 'PROPN', 'trace': 'NOUN', 'coronavirus': 'NOUN', 'origin': 'NOUN', 'europe': 'PROPN', 'china': 'PROPN', 'new': 'PROPN', 'york': 'PROPN', 'times': 'NOUN', 'one': 'NOUN', 'name': 'NOUN', 'source': 'NOUN', 'dog': 'PROPN'}\n"
     ]
    }
   ],
   "source": [
    "# run the code\n",
    "a = get_noun_spacy(document)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtree Matching for Relation Extraction using spacy\n",
    "def subtree_matcher(text):\n",
    "  nlp = get_spacy_model()\n",
    "  doc = nlp(text)\n",
    "  \n",
    "  subjpass = 0\n",
    "\n",
    "  for i,tok in enumerate(doc):\n",
    "    # find dependency tag that contains the text \"subjpass\"    \n",
    "    if tok.dep_.find(\"subjpass\") == True:\n",
    "      subjpass = 1\n",
    "\n",
    "  x = {}\n",
    "  y = {}\n",
    "  v = {}\n",
    "\n",
    "  # if subjpass == 1 then sentence is passive\n",
    "  if subjpass == 1:\n",
    "    for i,tok in enumerate(doc):\n",
    "      if tok.dep_.find(\"subjpass\") == True:\n",
    "        y[tok.dep_] = tok.text\n",
    "\n",
    "      if tok.dep_.endswith(\"obj\") == True:\n",
    "        x[tok.dep_] = tok.text\n",
    "  \n",
    "  # if subjpass == 0 then sentence is not passive\n",
    "  else:\n",
    "    for i,tok in enumerate(doc):\n",
    "      if tok.dep_.endswith(\"subj\") == True:\n",
    "        x[tok.dep_] = tok.text\n",
    "\n",
    "      if tok.dep_.endswith(\"obj\") == True:\n",
    "        y[tok.dep_] = tok.text\n",
    "  for i,tok in enumerate(doc):\n",
    "      if tok.dep_.endswith(\"ROOT\") == True:\n",
    "          v[tok.dep_] = tok.text  \n",
    "  return x,v,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "({'dobj': 'what', 'pobj': 'sad'}, {'ROOT': 'is'}, {'nsubjpass': 'they'})\n"
     ]
    }
   ],
   "source": [
    "# run Subtree Matching\n",
    "a = subtree_matcher(document)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GATE API call\n",
    "# import requests\n",
    "# url = \"https://cloud-api.gate.ac.uk/process-document/annie-named-entity-recognizer\"\n",
    "# headers = {'Content-Type': 'text/plain'}\n",
    "# response = requests.post(url, data=document, headers=headers).json()\n",
    "\n",
    "# import json\n",
    "# print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gate_ner(sentence):\n",
    "  return [(sentence[entity[\"indices\"][0]:entity[\"indices\"][1]] + f\" ({entity['gender']})\",entity_type) if entity_type == \"Person\" and \"gender\" in entity else (sentence[entity[\"indices\"][0]:entity[\"indices\"][1]],entity_type)  for entity_type,entities in requests.post(\"https://cloud-api.gate.ac.uk/process-document/annie-named-entity-recognizer\", data=sentence, headers={'Content-Type': 'text/plain'}).json()[\"entities\"].items() for entity in entities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('china', 'Location'),\n",
       " ('china', 'Location'),\n",
       " ('new york times', 'Organization')]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# GATE NER using API\n",
    "gate_ner(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK\n",
    "def nltk_ner(document):\n",
    "  return {(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "nltk_ner(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flair\n",
    "# having dependency issues\n",
    "def flair_ner(document, model):\n",
    "  from flair.data import Sentence\n",
    "  s = Sentence(document)\n",
    "  model.predict(s)\n",
    "  entities = s.to_dict(tag_type='ner')\n",
    "  return [(entity[\"text\"], entity[\"type\"]) for entity in entities[\"entities\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'type'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-04c49c1aba80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflair_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflair_4class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-aa27a47cac09>\u001b[0m in \u001b[0;36mflair_ner\u001b[0;34m(document, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-aa27a47cac09>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "# flair_ner(document, flair_4class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "'type'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-ff0ea2a9c040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mflair_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflair_12class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-aa27a47cac09>\u001b[0m in \u001b[0;36mflair_ner\u001b[0;34m(document, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-aa27a47cac09>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"entities\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "# flair_ner(document, flair_12class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "15578876784678163569 HelloWorld 0 3 Hello, world\n15578876784678163569 HelloWorld 4 6 Hello world\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern1 = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "pattern2 = [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n",
    "\n",
    "matcher.add(\"HelloWorld\", None, pattern1,pattern2)\n",
    "# matcher.add(\"HelloWorld\", None,\n",
    "#             [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}],\n",
    "#             [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}])\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'nytimes': 'ORG', 'europe': 'LOC', 'china': 'GPE', 'first': 'ORDINAL', 'new york times': 'ORG'}\n"
     ]
    }
   ],
   "source": [
    "# Get all NER\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "document= \"so now the fake news nytimes is trace the coronavirus origin back to europe not china this is a first i wonder what the fail new york times get for this one are there any name source they were recently throw out of china like dog and obviously want back in sad\"\n",
    "def get_ner_spacy(text):\n",
    "    if text:\n",
    "        doc = nlp(text)\n",
    "        ner_tags = {}\n",
    "        for ee in doc.ents:\n",
    "            ner_tags[ee.text] = ee.label_\n",
    "        return ner_tags\n",
    "\n",
    "# run the code\n",
    "a = get_ner_spacy(document_string)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "news NOUN\nnytimes PROPN\ntrace NOUN\ncoronavirus NOUN\norigin NOUN\neurope PROPN\nchina PROPN\nnew PROPN\nyork PROPN\ntimes NOUN\none NOUN\nname NOUN\nsource NOUN\nchina PROPN\ndog PROPN\n"
     ]
    }
   ],
   "source": [
    "# get all NOUNs and proper nouns\n",
    "doc = nlp(document)\n",
    "for token in doc:\n",
    "    if(token.pos_ == 'NOUN' or token.pos_ == 'PROPN'):\n",
    "        print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "nytimes 21 28 ORG\neurope 69 75 LOC\nchina 80 85 GPE\nfirst 96 101 ORDINAL\nnew york times 125 139 ORG\nchina 215 220 GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-70d6505f53b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Salesforce recently acquired Tableau.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoc_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtree_matcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtree_matcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tableau was recently acquired by Salesforce.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "({'dobj': 'what', 'pobj': 'sad'}, {'ROOT': 'is'}, {'nsubjpass': 'they'})\n"
     ]
    }
   ],
   "source": [
    "print(subtree_matcher(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  \"text\": \"so now the fake news nytimes is trace the coronavirus origin back to europe not china this is a first i wonder what the fail new york times get for this one are there any name source they were recently throw out of china like dog and obviously want back in sad\",\n  \"entities\": {\n    \"Location\": [\n      {\n        \"indices\": [\n          80,\n          85\n        ],\n        \"locType\": \"country\",\n        \"rule\": \"Location1\",\n        \"ruleFinal\": \"LocFinal\",\n        \"matches\": [\n          143,\n          145\n        ]\n      },\n      {\n        \"indices\": [\n          215,\n          220\n        ],\n        \"locType\": \"country\",\n        \"rule\": \"Location1\",\n        \"ruleFinal\": \"LocFinal\",\n        \"matches\": [\n          143,\n          145\n        ]\n      }\n    ],\n    \"Organization\": [\n      {\n        \"indices\": [\n          125,\n          139\n        ],\n        \"orgType\": \"company\",\n        \"rule\": \"GazOrganization\",\n        \"ruleFinal\": \"OrgFinal\"\n      }\n    ]\n  }\n}\n"
     ]
    }
   ],
   "source": [
    "document= \"so now the fake news nytimes is trace the coronavirus origin back to europe not china this is a first i wonder what the fail new york times get for this one are there any name source they were recently throw out of china like dog and obviously want back in sad\"\n",
    "import requests\n",
    "url = \"https://cloud-api.gate.ac.uk/process-document/annie-named-entity-recognizer\"\n",
    "headers = {'Content-Type': 'text/plain'}\n",
    "response = requests.post(url, data=document, headers=headers).json()\n",
    "import json\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sandipan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/sandipan/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package maxent_ne_chunker to\n[nltk_data]     /home/sandipan/nltk_data...\n[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n[nltk_data] Downloading package words to /home/sandipan/nltk_data...\n[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(\"so now the fake news nytimes is trace the coronavirus origin back to europe not china this is a first i wonder what the fail new york times get for this one are there any name source they were recently throw out of china like dog and obviously want back in sad\"))) if hasattr(chunk, 'label') }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting git+https://github.com/aboSamoor/polyglot.git@master\n",
      "  Cloning https://github.com/aboSamoor/polyglot.git (to revision master) to /tmp/pip-req-build-b0us1o72\n",
      "Collecting PyICU>=1.8\n",
      "  Downloading PyICU-2.6.tar.gz (233 kB)\n",
      "\u001b[K     |████████████████████████████████| 233 kB 7.2 MB/s \n",
      "\u001b[?25h\u001b[31m    ERROR: Command errored out with exit status 1:\n",
      "     command: '/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/nlp/bin/python3' -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-3ogsz3jz/pyicu/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-3ogsz3jz/pyicu/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' egg_info --egg-base /tmp/pip-pip-egg-info-vkle08a1\n",
      "         cwd: /tmp/pip-install-3ogsz3jz/pyicu/\n",
      "    Complete output (53 lines):\n",
      "    (running 'icu-config --version')\n",
      "    (running 'pkg-config --modversion icu-i18n')\n",
      "    Traceback (most recent call last):\n",
      "      File \"/tmp/pip-install-3ogsz3jz/pyicu/setup.py\", line 63, in <module>\n",
      "        ICU_VERSION = os.environ['ICU_VERSION']\n",
      "      File \"/usr/lib/python3.8/os.py\", line 675, in __getitem__\n",
      "        raise KeyError(key) from None\n",
      "    KeyError: 'ICU_VERSION'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"/tmp/pip-install-3ogsz3jz/pyicu/setup.py\", line 66, in <module>\n",
      "        ICU_VERSION = check_output(('icu-config', '--version')).strip()\n",
      "      File \"/tmp/pip-install-3ogsz3jz/pyicu/setup.py\", line 19, in check_output\n",
      "        return subprocess_check_output(popenargs)\n",
      "      File \"/usr/lib/python3.8/subprocess.py\", line 411, in check_output\n",
      "        return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "      File \"/usr/lib/python3.8/subprocess.py\", line 489, in run\n",
      "        with Popen(*popenargs, **kwargs) as process:\n",
      "      File \"/usr/lib/python3.8/subprocess.py\", line 854, in __init__\n",
      "        self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "      File \"/usr/lib/python3.8/subprocess.py\", line 1702, in _execute_child\n",
      "        raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "    FileNotFoundError: [Errno 2] No such file or directory: 'icu-config'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"/tmp/pip-install-3ogsz3jz/pyicu/setup.py\", line 69, in <module>\n",
      "        ICU_VERSION = check_output(('pkg-config', '--modversion', 'icu-i18n')).strip()\n",
      "      File \"/tmp/pip-install-3ogsz3jz/pyicu/setup.py\", line 19, in check_output\n",
      "        return subprocess_check_output(popenargs)\n",
      "      File \"/usr/lib/python3.8/subprocess.py\", line 411, in check_output\n",
      "        return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "      File \"/usr/lib/python3.8/subprocess.py\", line 489, in run\n",
      "        with Popen(*popenargs, **kwargs) as process:\n",
      "      File \"/usr/lib/python3.8/subprocess.py\", line 854, in __init__\n",
      "        self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "      File \"/usr/lib/python3.8/subprocess.py\", line 1702, in _execute_child\n",
      "        raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "    FileNotFoundError: [Errno 2] No such file or directory: 'pkg-config'\n",
      "    \n",
      "    During handling of the above exception, another exception occurred:\n",
      "    \n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"/tmp/pip-install-3ogsz3jz/pyicu/setup.py\", line 71, in <module>\n",
      "        raise RuntimeError('''\n",
      "    RuntimeError:\n",
      "    Please install pkg-config on your system or set the ICU_VERSION environment\n",
      "    variable to the version of ICU you have installed.\n",
      "    \n",
      "    ----------------------------------------\u001b[0m\n",
      "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
      "/bin/bash: polyglot: command not found\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'polyglot'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e64d14dfd921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip3 install -U git+https://github.com/aboSamoor/polyglot.git@master'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'polyglot download embeddings2.en ner2.en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpolyglot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'polyglot'"
     ]
    }
   ],
   "source": [
    "!pip3 install -U git+https://github.com/aboSamoor/polyglot.git@master\n",
    "!polyglot download embeddings2.en ner2.en\n",
    "from polyglot.text import Text\n",
    "Text(document).entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}