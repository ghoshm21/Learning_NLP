{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bitnlpd8d34a4971064c91a824973644f122aa",
   "display_name": "Python 3.8.5 64-bit ('nlp')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings #This module ignores the various types of warnings generated\n",
    "# warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import os #This module provides a way of using operating system dependent functionality\n",
    "\n",
    "import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
    "process = psutil.Process(os.getpid())\n",
    "from psutil import virtual_memory\n",
    "mem = virtual_memory()\n",
    "\n",
    "import time #This module is used to calculate the time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory used in GB before Loading the Model: 4.83\n",
      "----------\n",
      "37.54 seconds taken to load\n",
      "----------\n",
      "Finished loading Word2Vec\n",
      "----------\n",
      "Memory used in GB after Loading the Model: 4.87\n",
      "----------\n",
      "Percentage increase in memory usage: 100.84% \n",
      "----------\n",
      "Numver of words in vocablulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "# gensim embeddings\n",
    "# 1.word2vec\n",
    "# load using API downloader\n",
    "pre = process.memory_info().rss\n",
    "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "print('-'*10)\n",
    "\n",
    "start_time = time.time() #Start the timer\n",
    "ttl = mem.total #Toal memory available\n",
    "\n",
    "info = api.info()  # show info about available models/datasets\n",
    "model = api.load(\"word2vec-google-news-300\")  # download the model and return as object ready for use\n",
    "\n",
    "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "print('-'*10)\n",
    "\n",
    "print('Finished loading Word2Vec')\n",
    "print('-'*10)\n",
    "\n",
    "post = process.memory_info().rss\n",
    "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Numver of words in vocablulary: \",len(model.vocab)) #Number of words in the vocabulary. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('cats', 0.8099379539489746), ('dog', 0.7609456777572632), ('kitten', 0.7464985251426697), ('feline', 0.7326233983039856), ('beagle', 0.7150583267211914), ('puppy', 0.7075453996658325), ('pup', 0.6934291124343872), ('pet', 0.6891531348228455), ('felines', 0.6755931377410889), ('chihuahua', 0.6709762215614319)]\n",
      "[('gorgeous', 0.8353004455566406), ('lovely', 0.810693621635437), ('stunningly_beautiful', 0.7329413890838623), ('breathtakingly_beautiful', 0.7231341004371643), ('wonderful', 0.6854087114334106), ('fabulous', 0.6700063943862915), ('loveliest', 0.6612576246261597), ('prettiest', 0.6595001816749573), ('beatiful', 0.6593326330184937), ('magnificent', 0.6591402292251587)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"cat\"))\n",
    "print(model.most_similar('beautiful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pre trained model\n",
    "# !wget -P /tmp/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Memory used in GB before Loading the Model: 8.49\n",
      "----------\n",
      "41.32 seconds taken to load\n",
      "----------\n",
      "Finished loading Word2Vec\n",
      "----------\n",
      "Memory used in GB after Loading the Model: 13.15\n",
      "----------\n",
      "Percentage increase in memory usage: 154.78% \n",
      "----------\n",
      "Numver of words in vocablulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "# gensim embeddings\n",
    "# 1.word2vec\n",
    "# load already downloaded binary file\n",
    "# download the file\n",
    "model_file = \"/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/pre_trained_models/GoogleNews-vectors-negative300.bin.gz\"\n",
    "pre = process.memory_info().rss\n",
    "print(\"Memory used in GB before Loading the Model: %0.2f\"%float(pre/(10**9))) #Check memory usage before loading the model\n",
    "print('-'*10)\n",
    "\n",
    "start_time = time.time() #Start the timer\n",
    "ttl = mem.total #Toal memory available\n",
    "\n",
    "# Load the model\n",
    "w2v_model = KeyedVectors.load_word2vec_format(model_file, binary=True) #load the model\n",
    "\n",
    "print(\"%0.2f seconds taken to load\"%float(time.time() - start_time)) #Calculate the total time elapsed since starting the timer\n",
    "print('-'*10)\n",
    "\n",
    "print('Finished loading Word2Vec')\n",
    "print('-'*10)\n",
    "\n",
    "post = process.memory_info().rss\n",
    "print(\"Memory used in GB after Loading the Model: {:.2f}\".format(float(post/(10**9)))) #Calculate the memory used after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Percentage increase in memory usage: {:.2f}% \".format(float((post/pre)*100))) #Percentage increase in memory after loading the model\n",
    "print('-'*10)\n",
    "\n",
    "print(\"Numver of words in vocablulary: \",len(model.vocab)) #Number of words in the vocabulary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('cats', 0.8099379539489746), ('dog', 0.7609456777572632), ('kitten', 0.7464985251426697), ('feline', 0.7326233983039856), ('beagle', 0.7150583267211914), ('puppy', 0.7075453996658325), ('pup', 0.6934291124343872), ('pet', 0.6891531348228455), ('felines', 0.6755931377410889), ('chihuahua', 0.6709762215614319)]\n",
      "[('gorgeous', 0.8353004455566406), ('lovely', 0.810693621635437), ('stunningly_beautiful', 0.7329413890838623), ('breathtakingly_beautiful', 0.7231341004371643), ('wonderful', 0.6854087114334106), ('fabulous', 0.6700063943862915), ('loveliest', 0.6612576246261597), ('prettiest', 0.6595001816749573), ('beatiful', 0.6593326330184937), ('magnificent', 0.6591402292251587)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"cat\"))\n",
    "print(model.most_similar('beautiful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2b50c394e0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 2.glove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load using API downloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glove-twitter-200\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# download the model and return as object ready for use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/nlp/lib/python3.8/site-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/gensim-data/glove-twitter-200/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-twitter-200'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-twitter-200.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/nlp/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1545\u001b[0m         \"\"\"\n\u001b[1;32m   1546\u001b[0m         \u001b[0;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1547\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m             limit=limit, datatype=datatype)\n",
      "\u001b[0;32m~/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/nlp/lib/python3.8/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size)\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         logger.info(\n",
      "\u001b[0;32m~/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/nlp/lib/python3.8/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, result, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0m_add_word_to_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/nlp/lib/python3.8/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0m_add_word_to_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# gensim embeddings\n",
    "# 2.glove\n",
    "# load using API downloader\n",
    "model = api.load(\"glove-twitter-200\")  # download the model and return as object ready for use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(\"cat\"))\n",
    "print(model.most_similar('beautiful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to load already downloaded glove file??\n",
    "# glove_model = KeyedVectors.load_word2vec_format('/home/sandipan/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/pre_trained_models/glove-twitter-200.gz', binary=True) #load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
     ]
    }
   ],
   "source": [
    "fast_model = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('cats', 0.8368596434593201), ('housecat', 0.7674711346626282), ('-cat', 0.7602992057800293), ('dog', 0.7502298355102539), ('kitten', 0.7480818033218384), ('feline', 0.7353992462158203), ('super-cat', 0.7305205464363098), ('supercat', 0.7163283824920654), ('pet', 0.7090284824371338), ('moggy', 0.7057286500930786)]\n[('gorgeous', 0.8761377334594727), ('lovely', 0.8705508708953857), ('beautful', 0.8454244136810303), ('beautifull', 0.8216366767883301), ('beautiful-looking', 0.803433358669281), ('wonderful', 0.8029720783233643), ('beautiful-', 0.7908560633659363), ('magnificent', 0.7902208566665649), ('fabulous', 0.7820380926132202), ('beautyful', 0.7737030982971191)]\n"
     ]
    }
   ],
   "source": [
    "print(fast_model.most_similar(\"cat\"))\n",
    "print(fast_model.most_similar('beautiful'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-3.6987e-03  4.4550e-02  1.8869e-03  4.1758e-03 -1.8893e-01  9.8537e-02\n -8.6727e-03 -1.5412e-01 -2.7784e-02  5.8198e-02 -7.2687e-02  1.0579e-01\n -9.1134e-03 -1.8626e-02 -5.0600e-02 -8.3114e-02  1.3010e-01  3.1764e-02\n  1.1678e-01  1.5676e-03 -3.2579e-02  5.3644e-02 -3.2230e-03  1.0023e-01\n -6.8685e-02  3.1915e-02  4.9671e-02  4.1725e-02  6.3185e-02  4.3164e-02\n -8.9324e-03  6.1147e-02  1.8065e-02 -1.4466e-01  4.3917e-02  2.0808e-02\n  1.4150e-04 -7.6982e-04 -3.2587e-02 -3.1107e-03  7.0784e-02 -1.0393e-01\n -5.2313e-02  1.2775e-02 -1.4518e-02 -1.1491e-01 -3.1112e-02 -2.3520e-02\n  4.6074e-02  4.7045e-02 -1.0426e-01  1.2860e-02  1.8532e-02 -1.1636e-01\n -1.8211e-01  2.4853e-02  2.2081e-02 -1.3821e-02 -7.3555e-02 -3.4017e-02\n -4.5223e-02 -3.1666e-02  3.3307e-02  6.9280e-02 -2.7923e-02  1.8919e-02\n  2.7729e-03 -5.6738e-02  2.4467e-02 -4.7326e-02  4.1649e-02  1.6087e-02\n -3.4187e-03 -2.4747e-02  7.6686e-03 -1.6796e-02 -3.7160e-02 -1.1173e-01\n -6.9192e-02 -6.5262e-02  3.6255e-02  4.4894e-03  3.9682e-02  1.0369e-01\n  7.3057e-02  1.2581e-02 -2.0219e-02  5.5667e-03 -7.3201e-03  1.2677e-02\n  5.5060e-02 -4.3941e-02 -1.2456e-01 -8.9708e-02  6.4877e-02  2.0533e-02\n -1.1847e-01  2.0094e-02  5.3531e-02 -5.2026e-02  4.8369e-02 -3.8353e-04\n -2.1352e-02  6.7195e-02 -7.3223e-02 -1.5584e-01  1.0505e-02 -7.3986e-02\n  7.2704e-02 -6.5130e-02  2.9026e-02  2.1101e-01  7.3129e-02 -9.2135e-02\n -4.1537e-02  1.1085e-03 -5.0598e-03 -4.2303e-02 -8.2375e-02 -1.3038e-02\n  4.6585e-02  7.7033e-03 -3.9538e-03  9.6988e-02  1.2662e-01 -1.5377e-02\n -1.2112e-01 -4.8513e-02  1.1153e-02 -7.5772e-02 -1.2875e-01  7.8961e-02\n  7.0327e-02 -3.3189e-02 -2.8478e-02 -1.1616e-01  6.3316e-02 -1.4596e-02\n -8.4511e-02  2.5750e-02  3.8996e-02  4.5920e-04  1.2456e-03 -2.0537e-02\n  9.5769e-02  5.7098e-02 -5.8448e-02 -5.0685e-04  6.1715e-03  1.4405e-02\n  1.5806e-01  1.1391e-01  4.3870e-02 -8.4171e-02  1.5434e-01 -1.6339e-02\n -6.0682e-02 -5.9283e-02  1.7098e-02 -5.9405e-03  5.9562e-02  1.0897e-02\n  5.8010e-02  3.2960e-02  1.0930e-02  1.9353e-02  4.6436e-02 -4.6038e-02\n  1.8877e-02  4.7744e-02 -1.9678e-02  3.7382e-03  3.5239e-03  3.3411e-02\n -1.3083e-02  9.5171e-02 -3.6288e-02 -1.6699e-02  2.1268e-02  4.8605e-02\n -7.0773e-03 -3.1853e-02 -2.0670e-02 -1.1616e-01  1.3845e-02  2.6671e-02\n -1.2902e-02  1.7735e-01 -6.9101e-02 -7.6859e-02 -3.2152e-03  1.0715e-01\n  8.7976e-02  2.9357e-02 -3.6103e-02  7.0570e-02 -4.3275e-03 -4.0874e-02\n -2.3430e-02  4.2181e-02 -1.8609e-01  2.2322e-01  1.8956e-01 -9.1797e-02\n -1.9041e-02  2.7443e-02 -3.0005e-02 -2.6316e-02 -5.1210e-02 -9.7997e-02\n  4.7158e-03 -2.5797e-02  2.7367e-02  4.8455e-02  1.6084e-01 -4.1784e-02\n -2.3584e-02  2.6253e-02 -1.0640e-01 -1.5694e-01  6.7754e-02  3.2010e-02\n  1.4940e-02  1.6359e-01  9.2950e-02 -2.8457e-02  9.5104e-02  1.1479e-01\n  4.7012e-04 -7.0436e-03  2.1277e-02 -2.2126e-01 -1.0380e-01  9.5912e-03\n  2.1838e-01  3.2969e-02 -7.1673e-02  2.9995e-02 -5.2410e-02  3.6690e-02\n  2.6342e-01 -6.7078e-03 -1.9222e-02  2.6538e-02 -1.9433e-01  5.2930e-02\n -8.3401e-02 -1.5560e-01  2.5584e-02  1.4488e-03 -2.7691e-02  7.8949e-02\n -4.3348e-02 -1.1155e-02  7.6432e-02  1.2103e-02  1.2621e-02  3.6094e-02\n -1.9175e-02  1.3109e-01 -4.5976e-02  4.8244e-02  7.9931e-02 -7.9894e-03\n -1.0921e-01 -4.8242e-02 -1.5570e-02  9.8523e-02 -3.1253e-03  2.5260e-02\n -7.7714e-02  1.4908e-01 -1.3147e-01 -1.9370e-02  5.5517e-02 -2.1090e-01\n -2.0323e-02  2.9853e-02  2.0908e-02  7.4393e-02 -2.9707e-02  1.1693e-03\n  1.5060e-02 -1.8501e-02 -1.6579e-01 -7.1567e-02 -4.1917e-03 -1.2024e-02\n -3.0569e-02  1.2176e-02 -3.4802e-02 -1.1086e-02  3.8868e-02 -1.8663e-02\n  4.9803e-02  2.4127e-02  8.6400e-02 -2.2211e-02  1.7566e-02 -1.2369e-02]\n"
     ]
    }
   ],
   "source": [
    "# gensim print vector\n",
    "vector = fast_model['easy']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyError",
     "evalue": "\"word 'nice,' not in vocabulary\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-bb8d6c46aaae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# entire text is not easy, get word by word vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# spacy is good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfast_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"This is nice, we are learning Glove sandipan\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-bb8d6c46aaae>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# entire text is not easy, get word by word vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# spacy is good\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfast_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m\"This is nice, we are learning Glove sandipan\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/nlp/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, entities)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/nlp/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Insync/ghoshm21@gmail.com/Google Drive/personal_project/Learning_NLP/nlp/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'nice,' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# entire text is not easy, get word by word vector, also it will fail if the word does not present in the vocab\n",
    "# spacy is good\n",
    "vectors = [fast_model[x] for x in \"This is nice, we are learning Glove sandipan\".split(' ')]\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-8.7595e-02  3.5502e-01  6.3868e-02  2.9292e-01 -2.3635e-01 -6.2773e-02\n -1.6105e-01 -2.2842e-01  4.1587e-02  2.4844e+00 -3.8217e-01  3.2806e-02\n  1.2348e-01 -1.8422e-03 -1.3848e-01 -1.0005e-03 -4.3081e-02  1.1659e+00\n -4.7327e-02 -5.6004e-02  1.5617e-01 -1.3394e-01  2.3229e-01  8.7602e-02\n -3.2329e-01  1.6721e-01 -1.6221e-01 -9.1919e-02 -3.8004e-01  1.2686e-01\n  6.7819e-02  3.2509e-01 -5.7245e-02 -3.2630e-01 -1.1903e-01 -6.3964e-04\n -5.9275e-03 -2.9934e-01 -8.5043e-02 -2.6683e-01 -1.5815e-01  2.5963e-01\n  2.2571e-01  6.2582e-02 -1.9394e-01  2.1922e-01 -3.1186e-01  3.7084e-01\n -3.6577e-01 -5.2483e-02 -4.3101e-01  1.2379e-01  1.5529e-02 -1.2505e-01\n  2.2327e-01  2.9365e-01 -8.5104e-03 -8.3909e-02  2.4078e-01 -3.4913e-01\n -2.8355e-01 -7.6594e-02 -1.7130e-01  3.2869e-01  2.9024e-01 -6.2741e-02\n -5.5278e-02 -2.8706e-01  7.9608e-02  1.3234e-01  4.7857e-01  1.9623e-01\n  2.7314e-01 -1.3089e-01  2.7630e-01 -8.8846e-02 -1.2379e-01  7.3987e-02\n -5.1962e-01  3.5227e-01 -2.9182e-02  1.6203e-01 -3.6908e-02  2.8035e-01\n  3.1739e-01 -2.7597e-01 -4.3637e-01 -3.2842e-01  3.6760e-01 -1.6278e-01\n -1.6278e-01  3.7066e-01 -1.1340e-01  3.0920e-01  2.6133e-01  3.9483e-01\n -7.4612e-02 -2.2158e-01  2.5172e-01  2.9990e-01  1.0566e-01 -1.1406e-01\n -3.5395e-01  6.6704e-02  5.0216e-02 -7.1479e-01  9.8646e-02 -5.8832e-02\n -4.7790e-03 -2.3920e-01  1.0179e-01 -2.7205e-01  1.6836e-01  2.3420e-01\n -3.7496e-01  3.1125e-01 -3.1120e-01  2.1778e-01  3.0323e-01 -1.1729e-01\n -5.4639e-02 -1.5356e-01  5.1771e-02 -1.1426e-01  2.2473e-02  4.4405e-02\n -3.2101e-01 -2.7799e-01  2.4675e-01 -1.1760e-01 -1.5964e-02 -4.0969e-01\n -2.6082e-01  1.6021e-01  6.1166e-02 -3.1131e-03 -3.0573e-01 -4.1686e-02\n  2.3524e-01 -5.8415e-02 -1.6003e+00  1.0126e-01  1.2116e-01  1.5319e-02\n -1.1945e-01 -3.9095e-01 -1.9919e-01  4.3930e-02  2.2886e-01 -5.3961e-02\n -2.6570e-02  1.1952e-01  1.8446e-01 -6.9963e-02  3.6429e-01 -2.3679e-02\n -4.5081e-01 -3.7263e-02 -1.3243e-01 -1.1009e-01 -1.5201e-01  1.2182e-01\n -9.3379e-02  1.0215e-01 -3.4126e-01 -7.8150e-02  2.7685e-02 -4.8772e-03\n  2.7281e-01 -1.1304e-01  1.2470e-02  2.7008e-01  3.8885e-01 -2.3909e-01\n -1.6375e-01  1.9977e-02 -1.0628e-01  5.5798e-02  1.4127e-01  4.6536e-01\n -3.3169e-01  1.8308e-01  2.9646e-01  2.3906e-02  3.2799e-01 -5.3632e-01\n -4.6895e-01 -1.7593e-02  4.6805e-03 -9.6152e-02 -1.2695e-01  6.4099e-02\n -2.9787e-01  3.7799e-01  4.4469e-01  1.2248e-01  7.6388e-02 -1.8102e-01\n -1.1795e-02  4.0090e-01 -3.6967e-01 -2.4106e-01 -4.2252e-01  2.1378e-01\n  4.0977e-01  1.3013e-01 -3.3478e-02  9.3179e-03  2.9553e-01  6.8702e-02\n -1.4949e-01  1.0473e-01  3.8860e-01 -3.7063e-01 -6.8934e-02  4.2111e-01\n  1.0861e-01  1.8585e-01 -1.1387e-01  2.0370e-01  9.4214e-02 -2.1426e-01\n -1.9376e-01  1.2261e-01  1.3971e-01 -5.4205e-01 -2.4502e-01  4.7454e-01\n -5.9380e-02 -1.2865e-01 -1.7345e-01  3.7465e-01 -7.2616e-02  3.1124e-01\n -3.3315e-04 -3.1445e-03  1.1435e-03 -8.0773e-02 -5.2824e-02  2.6108e-01\n  2.4586e-01 -5.8762e-02 -2.1999e-02  1.4060e-01  3.6004e-01 -8.1521e-02\n -2.8828e-02  2.3878e-01 -1.2243e-01  1.8758e-01  6.9072e-02 -6.8685e-02\n  1.2377e-01  3.1713e-02 -1.1530e-01  3.8205e-01  4.8575e-01 -1.6466e-01\n  1.1033e-01  1.3873e-01  3.2145e-01  2.8014e-02 -5.4760e-02 -1.9438e-01\n -1.5438e-02 -5.6435e-02  2.3037e-02  4.2915e-01  5.9080e-01  9.6298e-02\n  1.2788e-01  1.5916e-01 -7.2056e-02 -2.1036e-01  5.7346e-02  4.5768e-02\n  1.5730e-01  3.1019e-01 -4.2692e-02  1.7531e-02  2.9585e-02 -4.2749e-02\n -1.0006e-01 -3.1611e-01 -4.5054e-03 -1.3689e-01  4.0798e-01 -6.5866e-02\n  2.0162e-01 -7.9660e-02 -3.9495e-02  9.3723e-02  9.3557e-02 -9.7551e-02\n  3.0639e-01 -2.7325e-01 -3.3112e-01  3.4460e-02 -1.5027e-01  4.0673e-01]\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Spacy Load the lg model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(\"This is nice, we are learning Glove sandipan\")\n",
    "#Averaged vector for the entire sentence\n",
    "# print(doc.vector) #Averaged vector for the entire sentence\n",
    "# get for only 1 word\n",
    "print(doc[0].vector) #1st word\n",
    "print(doc[8].vector) # last word, must be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}